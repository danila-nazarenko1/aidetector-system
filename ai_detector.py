import os
import requests
import json
import argparse
from tqdm import tqdm
from dotenv import load_dotenv
load_dotenv()


OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

MODEL = "qwen/qwen-2.5-7b-instruct"

MAX_CHARS = 6000

API_URL = "https://openrouter.ai/api/v1/chat/completions"


SYSTEM_ROLE = """
You are an experienced software forensics analyst and AI code detective.

Your task is to analyze source code and estimate
how likely it was generated by an AI system, including large language models like ChatGPT, Claude, or similar.

Important principles:

- Carefully weigh both AI and human indicators.
- Do not default to human authorship; consider clear AI patterns.
- Avoid over-speculation, but give AI probability when evidence is present.
- Focus on measurable coding patterns, style, and structure.
- Give a balanced, evidence-based assessment.

You are familiar with:

- Human coding styles, common inconsistencies, and imperfections
- AI code patterns: repetition, boilerplate, excessive uniformity
- Mixed human-AI workflows
- Comment style, naming conventions, and structural regularity
- Template-based and over-optimized code
"""



USER_PROMPT = """
Analyze the following source code and estimate
the probability that it was generated primarily by an AI system.

Instructions:

1. Look for observable patterns that suggest AI generation:
   - Repetitive and overly uniform code
   - Perfectly formatted and complete functions
   - Generic variable names or overly consistent naming
   - Boilerplate or template-like structures
   - Overly polished or explanatory comments
2. Take note of human indicators, but do not let them override strong AI evidence.
3. Provide a numerical ai_probability reflecting likelihood.
4. Provide a confidence level ("low", "medium", "high").
5. Give clear reasons for your judgment.

Return ONLY valid JSON in this format:

{{
  "ai_probability": number between 0 and 1,
  "confidence": "low" | "medium" | "high",
  "reasons": [string]
}}

Guidelines for ai_probability:

0.00 – 0.25 → Very likely human-written  
0.25 – 0.50 → Likely human, minor AI influence  
0.50 – 0.75 → Possible significant AI involvement  
0.75 – 1.00 → Likely primarily AI-generated

Code to analyze:
```python
{code}
"""


def split_into_chunks(text, size):
    chunks = []
    i = 0

    while i < len(text):
        chunks.append(text[i:i + size])
        i += size

    return chunks

def call_openrouter(system, user):
    headers = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "Content-Type": "application/json",
    "HTTP-Referer": "http://localhost",
    "X-Title": "AI Code Detector"
   }

    payload = {
        "model": MODEL,
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ],
        "temperature": 0,
        "max_tokens": 400
    }

    response = requests.post(
        API_URL,
        headers=headers,
        json=payload,
        timeout=60
    )

    if response.status_code != 200:
        raise Exception(f"API Error: {response.text}")

    data = response.json()

    return data["choices"][0]["message"]["content"]


def analyze_chunk(code_chunk):
    prompt = USER_PROMPT.format(code=code_chunk)

    response_text = call_openrouter(SYSTEM_ROLE, prompt)

    try:
        start = response_text.find("{")
        end = response_text.rfind("}") + 1

        json_part = response_text[start:end]

        return json.loads(json_part)

    except Exception:
        print("\n[WARNING] Invalid model response:")
        print(response_text)
        return None
    

def analyze_code(code_text):
    chunks = split_into_chunks(code_text, MAX_CHARS)

    print(f"Total chunks: {len(chunks)}")

    results = []

    for chunk in tqdm(chunks, desc="Analyzing"):
        res = analyze_chunk(chunk)

        if res:
            results.append(res)

    if not results:
        raise Exception("No valid responses from model")


    avg_score = sum(
        r["ai_probability"] for r in results
    ) / len(results)

    conf_map = {"low": 0, "medium": 1, "high": 2}
    inv_map = {0: "low", 1: "medium", 2: "high"}

    avg_conf = round(
        sum(conf_map[r["confidence"]] for r in results) / len(results)
    )

    reasons = []

    for r in results:
        reasons.extend(r["reasons"])

    main_reasons = list(dict.fromkeys(reasons))[:6]


    return {
        "final_ai_probability": round(avg_score, 3),
        "confidence": inv_map[avg_conf],
        "chunks_analyzed": len(results),
        "main_reasons": main_reasons
    }


def main():
    parser = argparse.ArgumentParser(
    description="AI-generated code detector (OpenRouter)"
    )

    parser.add_argument(
        "--file",
        help="Read code from file"
    )

    parser.add_argument(
        "--stdin",
        action="store_true",
        help="Read code from stdin"
    )

    args = parser.parse_args()



    if not OPENROUTER_API_KEY:
        print("ERROR: OPENROUTER_API_KEY not set")
        return

    if args.file:

        with open(args.file, "r", encoding="utf-8") as f:
            code = f.read()

    elif args.stdin:

        print("Paste code, finish with CTRL+D:\n")

        code = ""

        try:
            while True:
                line = input()
                code += line + "\n"

        except EOFError:
            pass

    else:
        print("Use --file or --stdin")
        return


    if not code.strip():
        print("ERROR: Empty input")
        return


    print("\nInput size:", len(code), "characters")
    print("Starting analysis...\n")


    result = analyze_code(code)


    print("\n========== RESULT ==========\n")

    print(json.dumps(result, indent=2))

    print("\n============================\n")

def detect_from_code(code: str) -> dict:
    """
    Detect AI probability from raw code string.
    Used by pipeline.
    """
    if not code.strip():
        return None

    return analyze_code(code)

if __name__ == "__main__":
    main()